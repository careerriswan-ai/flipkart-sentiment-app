{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1105c345-fb5f-41bb-a7a5-e77b9168dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6eb5e0-b2be-43c2-a0d1-27b0bc527d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7895 reviews\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "print(f\"Loaded {len(df)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b6bf60-feac-46b3-a255-2b1fff166814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE cleaning:\n",
      "Nice product, good quality, but price is now rising which is a bad sign. 800-850 was an affordable price, especially when we play everyday. So kindly help us out in terms of the price. Thank You.READ MORE\n",
      "\n",
      "AFTER cleaning:\n",
      "nice product good quality but price is now rising which is a bad sign was an affordable price especially when we play everyday so kindly help us out in terms of the price thank youread more\n"
     ]
    }
   ],
   "source": [
    "#Clean Text function\n",
    "def clean_text(text):\n",
    "        \n",
    "    if pd.isna(text):  \n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()  \n",
    "    text = re.sub(r'http\\S+', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "sample = df['Review text'].iloc[0]\n",
    "print(\"BEFORE cleaning:\")\n",
    "print(sample)\n",
    "print(\"\\nAFTER cleaning:\")\n",
    "print(clean_text(sample))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac9c5ab-9cf9-4475-b3d2-44552b242d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all reviews... (this takes a minute)\n",
      "Done!\n",
      "\n",
      "--- Example 1 ---\n",
      "ORIGINAL: Nice product, good quality, but price is now rising which is a bad sign. 800-850...\n",
      "CLEANED:  nice product good quality but price is now rising which is a bad sign was an aff...\n",
      "\n",
      "--- Example 2 ---\n",
      "ORIGINAL: They didn't supplied Yonex Mavis 350. Outside cover was Yonex Ad inside was a ch...\n",
      "CLEANED:  they didnt supplied yonex mavis outside cover was yonex ad inside was a cheapest...\n",
      "\n",
      "--- Example 3 ---\n",
      "ORIGINAL: Worst product. Damaged shuttlecocks packed in new box. It's not a original yonex...\n",
      "CLEANED:  worst product damaged shuttlecocks packed in new box its not a original yonex pr...\n"
     ]
    }
   ],
   "source": [
    "#Clean All reviews\n",
    "# Apply cleaning to ALL reviews\n",
    "print(\"Cleaning all reviews... (this takes a minute)\")\n",
    "\n",
    "df['Cleaned_Text'] = df['Review text'].apply(clean_text)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"ORIGINAL: {df['Review text'].iloc[i][:80]}...\")\n",
    "    print(f\"CLEANED:  {df['Cleaned_Text'].iloc[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a337e08-cc76-4aed-8e72-a7ebc404b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rriya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889ddd78-0332-42c6-b823-49ed43e2cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67331b2-f528-4d71-bc16-560600caf299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Removing stopwords...\n",
      "\n",
      "Example:\n",
      "WITH stopwords:    nice product good quality but price is now rising which is a bad sign was an affordable price especially when we play everyday so kindly help us out in terms of the price thank youread more\n",
      "WITHOUT stopwords: nice good quality price rising bad sign affordable price especially play everyday kindly help us terms price thank youread\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "stop_words.update(['flipkart', 'product', 'read', 'more'])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "print(\" Removing stopwords...\")\n",
    "df['No_Stopwords'] = df['Cleaned_Text'].apply(remove_stopwords)\n",
    "\n",
    "# Show difference\n",
    "print(\"\\nExample:\")\n",
    "print(f\"WITH stopwords:    {df['Cleaned_Text'].iloc[0]}\")\n",
    "print(f\"WITHOUT stopwords: {df['No_Stopwords'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0690719d-1ffe-4749-ae3c-e6f6cee210ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rriya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rriya\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baf7c6b3-5852-4c14-9f85-be5fbdff81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b9bdab7-f86a-4dbe-89a4-444aa82b92ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing... (this takes a minute)\n",
      "Done!\n",
      "\n",
      "Full transformation example:\n",
      "ORIGINAL:     Nice product, good quality, but price is now rising which is a bad sign. 800-850 was an affordable price, especially when we play everyday. So kindly help us out in terms of the price. Thank You.READ MORE\n",
      "CLEANED:      nice product good quality but price is now rising which is a bad sign was an affordable price especially when we play everyday so kindly help us out in terms of the price thank youread more\n",
      "NO STOPWORDS: nice good quality price rising bad sign affordable price especially play everyday kindly help us terms price thank youread\n",
      "LEMMATIZED:   nice good quality price rising bad sign affordable price especially play everyday kindly help u term price thank youread\n"
     ]
    }
   ],
   "source": [
    "# Lemmatiztion\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "print(\"Lemmatizing... (this takes a minute)\")\n",
    "df['Final_Text'] = df['No_Stopwords'].apply(lemmatize_text)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "print(\"\\nFull transformation example:\")\n",
    "print(f\"ORIGINAL:     {df['Review text'].iloc[0]}\")\n",
    "print(f\"CLEANED:      {df['Cleaned_Text'].iloc[0]}\")\n",
    "print(f\"NO STOPWORDS: {df['No_Stopwords'].iloc[0]}\")\n",
    "print(f\"LEMMATIZED:   {df['Final_Text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4dbe77-d0b7-4751-bc49-67b6a6f637d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4983 very short reviews\n",
      "Remaining: 2912 reviews\n"
     ]
    }
   ],
   "source": [
    "#Remove Short Reviews\n",
    "df['Word_Count'] = df['Final_Text'].apply(lambda x: len(x.split()))\n",
    "df_final = df[df['Word_Count'] >= 3].copy()\n",
    "\n",
    "print(f\"Removed {len(df) - len(df_final)} very short reviews\")\n",
    "print(f\"Remaining: {len(df_final)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c1ccbe-dc5e-415a-a529-71d06b779798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset ready!\n",
      "                                              Review Sentiment  Label\n",
      "0  nice good quality price rising bad sign afford...  Positive      1\n",
      "1  didnt supplied yonex mavis outside cover yonex...  Negative      0\n",
      "2  worst damaged shuttlecock packed new box origi...  Negative      0\n",
      "3  pricedjust retaileri didnt understand wat adva...  Negative      0\n",
      "4                    good quality delivered timeread  Positive      1\n",
      " Saved to 'preprocessed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# final clean dataset\n",
    "\n",
    "final_df = df_final[['Final_Text', 'Sentiment']].copy()\n",
    "final_df.columns = ['Review', 'Sentiment']\n",
    "\n",
    "final_df['Label'] = final_df['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "print(\"Final dataset ready!\")\n",
    "print(final_df.head())\n",
    "\n",
    "\n",
    "final_df.to_csv('preprocessed_data.csv', index=False)\n",
    "print(\" Saved to 'preprocessed_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f931e-4f1e-4fdf-b0c0-bf8ee4e187e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
